{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQIQkuPQz+2ou7szXdz07L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annek77/llamaindex-chat-with-streamlit-docs/blob/main/Party_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpYNtGr7KlE0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. System Imports (Path Handling & Dynamic Filenames)\n",
        "\n",
        "**Purpose:** Provide OS-level utilities for reliable file and environment management.\n",
        "\n",
        "- **WHAT:** Import Pythonâ€™s built-in `os` module.  \n",
        "- **WHY:** Enables cross-platform path construction, directory creation, existence checks, and secure access to environment variables.  \n",
        "- **HOW:** Throughout the pipeline we will use `os.path.join`, `os.makedirs`, `os.path.exists`, `os.getenv()` etc.\n"
      ],
      "metadata": {
        "id": "q_jK4SLXLJkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os module for operating-system interactions\n",
        "# Enables robust path handling, directory creation, existence checks, and secure environment variable access\n",
        "import os\n"
      ],
      "metadata": {
        "id": "6u_rqEB0LLFy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. System Imports (Path Handling & Dynamic Filenames)\n",
        "\n",
        "**Purpose:** Provide essential OS-level utilities to guarantee consistent file operations and environment configuration across the entire pipeline.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  import os\n",
        "  ```\n",
        "\n",
        "  Load Pythonâ€™s built-in `os` module.\n",
        "\n",
        "* **Why:**\n",
        "  Enables cross-platform file and directory management, path construction, existence checks, and secure access to environment variables.\n",
        "\n",
        "* **How:**\n",
        "  Use\n",
        "\n",
        "  * `os.path.join()` to assemble file paths\n",
        "  * `os.makedirs()` to create required directories\n",
        "  * `os.path.exists()` to verify file or folder presence\n",
        "  * `os.getenv()` to fetch environment variables (e.g. API keys) without hard-coding\n"
      ],
      "metadata": {
        "id": "dJZlSGHsbcoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the PDF to analyze and backup settings\n",
        "# Enables dynamic PDF selection, optional text backup, and controlled extraction stopping\n",
        "ACTIVE_PDF = \"Introduction_Why_Data_Science_Needs_Feminism.pdf\"\n",
        "EXTRACTED_TEXT_FILE = \"extracted_text.txt\"\n",
        "EXTRACT_MARKERS = [\"references\", \"bibliography\"]\n"
      ],
      "metadata": {
        "id": "Co6nu0KPbeFQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J1S4SbZQbtGA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TaoknTrYfG80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. PDF Settings\n",
        "\n",
        "**Purpose:** Define the source PDF file and control text extraction parameters.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  ACTIVE_PDF = \"Introduction_Why_Data_Science_Needs_Feminism.pdf\"\n",
        "  EXTRACTED_TEXT_FILE = \"extracted_text.txt\"\n",
        "  EXTRACT_MARKERS = [\"references\", \"bibliography\"]\n",
        "  ```\n",
        "\n",
        "  Specify the input PDF filename, optional backup text file, and extraction stop markers.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Allows switching to different PDF inputs without modifying code.\n",
        "  * Ensures raw extracted text is saved for debugging or further analysis.\n",
        "  * Prevents inclusion of unwanted sections (e.g., references or bibliography) in the main text.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  * Use `ACTIVE_PDF` to locate and open the target PDF.\n",
        "  * Write full extracted text to `EXTRACTED_TEXT_FILE` when needed.\n",
        "  * Halt extraction upon encountering any term in `EXTRACT_MARKERS` during page processing.\n"
      ],
      "metadata": {
        "id": "ivrMHuUwfH2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure spaCy model and pipeline components\n",
        "# Use lightweight small model for speed and memory efficiency; disable parser to reduce computation\n",
        "SPACY_MODEL = \"en_core_web_sm\"\n",
        "SPACY_DISABLE = [\"parser\"]\n",
        "SPACY_BATCH_SIZE = 1000\n",
        "\n",
        "# Define output path for tokenization JSON\n",
        "# Persist tokenized data for debugging and further analysis\n",
        "TOKENS_JSON_OUTPUT = \"tokens_step2.json\"\n"
      ],
      "metadata": {
        "id": "gHjYa9jJfKx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenization & Linguistics (spaCy)\n",
        "\n",
        "**Purpose:** Configure the NLP pipeline to efficiently process and structure the extracted text.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  SPACY_MODEL = \"en_core_web_sm\"\n",
        "  SPACY_DISABLE = [\"parser\"]\n",
        "  SPACY_BATCH_SIZE = 1000\n",
        "  TOKENS_JSON_OUTPUT = \"tokens_step2.json\"\n",
        "  ```\n",
        "\n",
        "  Select the spaCy model, disable unused components, set batch size, and define the JSON export path for tokenized data.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Use the lightweight `en_core_web_sm` model to balance speed and accuracy in tokenization, POS-tagging, and NER.\n",
        "  * Disable the parser to reduce computational overhead when dependency parsing is not required.\n",
        "  * Process text in batches of 1,000 tokens to optimize memory usage and throughput.\n",
        "  * Export token data to a JSON file for downstream analysis, debugging, and reproducibility.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  1. Load the spaCy model via `spacy.load(SPACY_MODEL, disable=SPACY_DISABLE)`.\n",
        "  2. Iterate over text in chunks of `SPACY_BATCH_SIZE` tokens using `nlp.pipe(...)`.\n",
        "  3. Serialize each documentâ€™s tokens and annotations to `TOKENS_JSON_OUTPUT`.\n"
      ],
      "metadata": {
        "id": "0tm3VOubfT4U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5jmA9VdSfa0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CSV export columns for token data\n",
        "# Ensure consistent ordering across all CSV outputs for correct parsing\n",
        "CSV_COLUMNS = [\n",
        "    \"word\", \"lemma\", \"pos\", \"ner\",\n",
        "    \"cefr_predicted\", \"translation_de\",\n",
        "    \"anki_front\", \"anki_back\", \"anki_type\",\n",
        "    \"status_learning\", \"repetition_stage\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "d7gksIIYfa2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CSV Export: Columns for Token Data\n",
        "\n",
        "**Purpose:** Define a consistent schema for CSV outputs to ensure correct parsing and interoperability across the pipeline.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  CSV_COLUMNS = [\n",
        "      \"word\", \"lemma\", \"pos\", \"ner\",\n",
        "      \"cefr_predicted\", \"translation_de\",\n",
        "      \"anki_front\", \"anki_back\", \"anki_type\",\n",
        "      \"status_learning\", \"repetition_stage\"\n",
        "  ]\n",
        "  ```\n",
        "\n",
        "  Central list of column names used in all generated CSV files.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Maintains a uniform column order in every CSV export, preventing misalignment when loading data.\n",
        "  * Simplifies maintenance by defining column schema in a single location, avoiding duplication.\n",
        "  * Enhances readability and clarity of data structure for further analysis and tool integration.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  * Reference `CSV_COLUMNS` in export scripts (e.g., `pandas.DataFrame.to_csv(columns=CSV_COLUMNS)`).\n",
        "  * Update this list as new fields are added to the processing pipeline to automatically propagate changes to all outputs.\n"
      ],
      "metadata": {
        "id": "MN-If68Yfhpc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LU7jDcAefqlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Translation Configuration\n",
        "\n",
        "**Purpose:** Define translation service preferences, fallback behavior, and caching mechanism to ensure reliable and efficient translations.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  TRANSLATION_SERVICE = \"wiktionary\"\n",
        "  ENABLE_FALLBACK = True\n",
        "  FALLBACK_SERVICE = \"libre\"\n",
        "  TRANSLATION_CACHE_FILE = \"translation_cache.json\"\n",
        "  TRANSLATE_TARGET = \"token\"\n",
        "  ```\n",
        "\n",
        "  Configure primary offline translation source, enable fallback to an online API, specify cache file, and set translation granularity.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Word translations are essential for creating bilingual vocabulary entries.\n",
        "  * Offline-first approach reduces dependence on external APIs and speeds up common lookups.\n",
        "  * Fallback to LibreTranslate ensures coverage if offline lookup fails.\n",
        "  * Caching avoids redundant API calls and accelerates repeated translations.\n",
        "  * Translation target controls whether tokens or entire vocabulary entries are translated.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  * Use the `TRANSLATION_SERVICE` setting to choose between WiktionaryParser and fallback API.\n",
        "  * Check `ENABLE_FALLBACK` and call `_translate_api` when offline lookup yields no result.\n",
        "  * Store and retrieve translations from `TRANSLATION_CACHE_FILE` to minimize network requests.\n",
        "  * Apply translations at the token level as specified by `TRANSLATE_TARGET`.\n"
      ],
      "metadata": {
        "id": "I-LxZeO_f0sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 5: Export Files (Dynamic Naming)\n",
        "# Output filenames are derived from the active PDF name.\n",
        "# No fixed constants are defined here; generated names follow the pattern:\n",
        "#    <PDF_BASE_NAME>_<descriptor>.csv, .txt, etc.\n",
        "# Example: \"Introduction_Why_Data_Science_Needs_Feminism_all_tokens.csv\"\n"
      ],
      "metadata": {
        "id": "RhhUvFj6f2_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Export Files (Dynamic Naming)\n",
        "\n",
        "**Purpose:** Automatically derive output filenames from the active PDF name to maintain consistency and avoid manual renaming.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  * Dynamic naming pattern for all export files (tokens, CSVs, Anki decks, metadata).\n",
        "  * No hard-coded filenames; base names are constructed from `ACTIVE_PDF`.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Ensures uniform naming conventions across different export types.\n",
        "  * Reduces risk of filename collisions and manual errors.\n",
        "  * Simplifies batch processing when handling multiple PDFs.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  * Extract the base name from `ACTIVE_PDF` (e.g., `Introduction_Why_Data_Science_Needs_Feminism`).\n",
        "  * Append descriptors (e.g., `_all_tokens.csv`, `_vocab_list.csv`).\n",
        "  * Use `os.path.join()` to construct full paths for each output file in the desired directory structure.\n"
      ],
      "metadata": {
        "id": "X8QH9QyCf9hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 6: Interactive Quiz & Anki Control\n",
        "# Enable or disable the terminal quiz and Anki deck generation\n",
        "ENABLE_TERMINAL_QUIZ = True\n",
        "EXPORT_ANKI_DECK = True\n"
      ],
      "metadata": {
        "id": "y_N03nbAgCqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Interactive Quiz & Anki Control\n",
        "\n",
        "**Purpose:** Configure whether to activate the terminal-based vocabulary quiz and whether to generate Anki flashcard decks for spaced repetition.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  ENABLE_TERMINAL_QUIZ = True\n",
        "  EXPORT_ANKI_DECK = True\n",
        "  ```\n",
        "\n",
        "  Toggle flags for enabling the built-in terminal quiz and exporting Anki decks.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Provides an interactive practice mode directly in the console for quick vocabulary reinforcement.\n",
        "  * Generates Anki decks to support long-term retention through spaced repetition.\n",
        "  * Allows flexibility to disable one or both features during development or batch processing.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  * Check `ENABLE_TERMINAL_QUIZ` before launching the quiz loop.\n",
        "  * Use `EXPORT_ANKI_DECK` to conditionally call the Anki deck export function (e.g., via `genanki`).\n",
        "  * Maintain separation between data extraction and learning interface to ensure modularity.\n"
      ],
      "metadata": {
        "id": "q7d7U3NogQ5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 7: Vocabulary Filters, Stopwords & Phrases\n",
        "# Configure custom stopwords, phrase lists, and raw vocabulary sources\n",
        "EXTRA_STOPWORDS = []\n",
        "PHRASE_LIST_PATH = \"phrases.txt\"\n",
        "RAW_VOCAB_LIST_PATHS = [\n",
        "    \"The_Oxford_5000_by_CEFR_level.pdf\",\n",
        "    \"digital_humanities_terms.txt\"\n",
        "]\n",
        "VOCAB_LIST_OUTPUT = \"vocab_list.csv\"\n"
      ],
      "metadata": {
        "id": "0urVHxjhgb-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Vocabulary Filters, Stopwords & Phrases\n",
        "\n",
        "**Purpose:** Configure filtering of tokens using custom stopwords and phrase lists, and specify raw vocabulary sources for list construction.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  EXTRA_STOPWORDS = []\n",
        "  PHRASE_LIST_PATH = \"phrases.txt\"\n",
        "  RAW_VOCAB_LIST_PATHS = [\n",
        "      \"The_Oxford_5000_by_CEFR_level.pdf\",\n",
        "      \"digital_humanities_terms.txt\"\n",
        "  ]\n",
        "  VOCAB_LIST_OUTPUT = \"vocab_list.csv\"\n",
        "  ```\n",
        "\n",
        "  Define additional stopwords to exclude, path to custom phrase list, sources for raw vocabulary extraction, and output file for consolidated vocabulary list.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Exclude irrelevant or high-frequency words via `EXTRA_STOPWORDS`.\n",
        "  * Identify multi-word expressions using a phrase list for more accurate token grouping.\n",
        "  * Incorporate external vocabulary sources (e.g., Oxford 5000 list, domain-specific terms) to enrich the final list.\n",
        "  * Export filtered and merged vocabulary as a CSV for downstream quiz and flashcard generation.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  * Load `EXTRA_STOPWORDS` and remove matching tokens during filtering.\n",
        "  * Read phrases from `PHRASE_LIST_PATH` and detect them in the token stream.\n",
        "  * Iterate through each path in `RAW_VOCAB_LIST_PATHS`, parse entries, and merge lists.\n",
        "  * Write the consolidated list to `VOCAB_LIST_OUTPUT` using `pandas.DataFrame.to_csv` with predefined `CSV_COLUMNS`.\n"
      ],
      "metadata": {
        "id": "ZLymg5g6gcyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 8: Metadata & Source Management\n",
        "# Configure metadata fields and output files for citations and vocabulary identifiers\n",
        "DUBLIN_CORE_FIELDS = [\n",
        "    \"title\", \"creator\", \"publisher\", \"date\",\n",
        "    \"format\", \"identifier\", \"language\"\n",
        "]\n",
        "DUBLIN_CORE_OUTPUT = \"dublin_core_metadata.txt\"\n",
        "APA_CITATION_OUTPUT = \"citation_APA.txt\"\n",
        "VOCAB_ID_PREFIX = \"vocab_\"\n",
        "SOURCE_ID_PREFIX = \"source_\"\n"
      ],
      "metadata": {
        "id": "LKLNn70JgvOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Metadata & Source Management\n",
        "\n",
        "**Purpose:** Define citation metadata fields and identifier prefixes for vocabulary entries and data sources, ensuring standardized reference handling and export.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  DUBLIN_CORE_FIELDS = [\n",
        "      \"title\", \"creator\", \"publisher\", \"date\",  \n",
        "      \"format\", \"identifier\", \"language\"\n",
        "  ]\n",
        "  DUBLIN_CORE_OUTPUT = \"dublin_core_metadata.txt\"\n",
        "  APA_CITATION_OUTPUT = \"citation_APA.txt\"\n",
        "  VOCAB_ID_PREFIX = \"vocab_\"\n",
        "  SOURCE_ID_PREFIX = \"source_\"\n",
        "  ```\n",
        "\n",
        "  Specify the metadata fields for Dublin Core, output filenames for metadata and APA citations, and prefixes for generated IDs.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Ensure consistent metadata formatting across all exports.\n",
        "  * Automate creation of standardized citation files for documentation and publication.\n",
        "  * Generate clear, unique identifiers for vocabulary items and sources, avoiding naming conflicts.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  * Iterate over `DUBLIN_CORE_FIELDS` to build metadata entries in the correct order.\n",
        "  * Write metadata lines to `DUBLIN_CORE_OUTPUT` and log APA-formatted citation to `APA_CITATION_OUTPUT`.\n",
        "  * Prepend `VOCAB_ID_PREFIX` and `SOURCE_ID_PREFIX` to each generated identifier when exporting vocabulary or citation data.\n"
      ],
      "metadata": {
        "id": "SjkVQrFsgvx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 9: Debug & Logging\n",
        "# Configure debug mode and logging outputs for development and troubleshooting\n",
        "DEBUG_MODE = True\n",
        "LOG_TO_CONSOLE = True\n",
        "LOG_TO_FILE = False\n"
      ],
      "metadata": {
        "id": "rzthKtFsg86j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Debug & Logging\n",
        "\n",
        "**Purpose:** Configure debugging behavior and logging destinations to facilitate development, troubleshooting, and audit trail creation.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  DEBUG_MODE = True\n",
        "  LOG_TO_CONSOLE = True\n",
        "  LOG_TO_FILE = False\n",
        "  ```\n",
        "\n",
        "  Toggle flags for enabling verbose debug output and choosing between console or file logging.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Activate `DEBUG_MODE` to print detailed internal state and error messages during development.\n",
        "  * Use `LOG_TO_CONSOLE` for immediate feedback in the terminal.\n",
        "  * Enable `LOG_TO_FILE` to persist logs for later analysis or auditing.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  * Wrap key functions with conditional logging statements based on `DEBUG_MODE`.\n",
        "  * Route log messages to stdout when `LOG_TO_CONSOLE` is `True`.\n",
        "  * If `LOG_TO_FILE` is `True`, write log entries to a configured log file using Pythonâ€™s `logging` module with a file handler.\n",
        "\n"
      ],
      "metadata": {
        "id": "pUt7OlCeg9mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages for PDF processing\n",
        "%pip install tools\n",
        "%pip install PyMuPDF\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS1-ryRLkcx9",
        "outputId": "c6bf2cb1-d47a-4d0b-d09c-7e0f38d4bf5a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tools\n",
            "  Downloading tools-1.0.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading tools-1.0.2-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: tools\n",
            "Successfully installed tools-1.0.2\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n",
        "\n",
        "**Purpose:** Ensure required packages are installed for PDF processing and text sanitization.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  %pip install tools\n",
        "  %pip install PyMuPDF\n",
        "  ```\n",
        "\n",
        "  Install the `tools` utility package and the `PyMuPDF` library for PDF parsing.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * `tools` offers helper functions used across the project.\n",
        "  * `PyMuPDF` (imported as `fitz`) provides reliable text extraction capabilities from PDF documents.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  * Run the `%pip install` commands in the first cell of the notebook.\n",
        "  * Use Colabâ€™s magic commands to ensure packages install into the active environment without restarting the runtime.\n",
        "\n"
      ],
      "metadata": {
        "id": "kWr2u4FWknny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 10: PDF Extraction & Sanitization\n",
        "# Install required packages for PDF processing\n",
        "%pip install tools\n",
        "%pip install PyMuPDF\n",
        "\n",
        "# Import libraries for PDF parsing and text cleaning\n",
        "import fitz  # PyMuPDF for PDF reading\n",
        "import re    # Regular expressions for text sanitization\n",
        "\n",
        "# Function: sanitize_text\n",
        "# Remove footnote markers, back-reference arrows, and extraneous lines\n",
        "def sanitize_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean the raw text by:\n",
        "    1. Removing bracketed footnote numbers ([1], [23], â€¦)\n",
        "    2. Stripping back-reference symbols (â†©)\n",
        "    3. Dropping lines composed solely of digits\n",
        "    4. Excluding lines like 'Page 7' or 'Seite 8'\n",
        "    5. Skipping lines starting with numbered lists (e.g., '1. ', '2. ')\n",
        "    \"\"\"\n",
        "    # (1) Remove bracketed footnote numbers\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    # (2) Strip back-reference arrows\n",
        "    text = text.replace('â†©', '')\n",
        "\n",
        "    filtered_lines = []\n",
        "    for line in text.splitlines():\n",
        "        stripped = line.strip()\n",
        "        # (3) Skip lines composed only of digits\n",
        "        if re.fullmatch(r'\\d+', stripped):\n",
        "            continue\n",
        "        # (4) Skip page labels like 'Page 7' or 'Seite 8'\n",
        "        if re.fullmatch(r'(page|seite)\\s*\\d+', stripped.lower()):\n",
        "            continue\n",
        "        # (5) Skip lines starting with numbered lists\n",
        "        if re.match(r'^\\d+\\.\\s', stripped):\n",
        "            continue\n",
        "        filtered_lines.append(line)\n",
        "    return \"\\n\".join(filtered_lines)\n",
        "\n",
        "# Main function: extract_clean_text\n",
        "# Parse PDF until a marker is found, then sanitize and optionally save\n",
        "def extract_clean_text(\n",
        "    pdf_path: str = ACTIVE_PDF,\n",
        "    markers: list = EXTRACT_MARKERS,\n",
        "    save_to_file: bool = False\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Read text from a PDF, stop at defined markers, sanitize content, and optionally write to file.\n",
        "    \"\"\"\n",
        "    full_text = \"\"\n",
        "    stops = [m.lower() for m in markers] + ['footnotes']\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error opening PDF '{pdf_path}': {e}\")\n",
        "\n",
        "    try:\n",
        "        for page in doc:\n",
        "            txt = page.get_text(\"text\")\n",
        "            low = txt.lower()\n",
        "            hits = [(low.find(m), m) for m in stops if m in low]\n",
        "            if hits:\n",
        "                idx, mark = min(hits, key=lambda x: x[0])\n",
        "                full_text += txt[:idx]\n",
        "                print(f\"Marker '{mark}' found on page {page.number+1}, stopping extraction.\")\n",
        "                break\n",
        "            full_text += txt\n",
        "    finally:\n",
        "        doc.close()\n",
        "\n",
        "    # Clean the concatenated text\n",
        "    clean = sanitize_text(full_text)\n",
        "\n",
        "    # Optionally save to file\n",
        "    if save_to_file:\n",
        "        try:\n",
        "            with open(EXTRACTED_TEXT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(clean)\n",
        "            print(f\"Text saved to: {EXTRACTED_TEXT_FILE}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing file '{EXTRACTED_TEXT_FILE}': {e}\")\n",
        "    return clean\n",
        "\n",
        "# Script entry point\n",
        "if __name__ == \"__main__\":\n",
        "    result = extract_clean_text(save_to_file=True)\n",
        "    print(\"\\n--- Preview (first 500 characters) ---\\n\")\n",
        "    print(result[:500])\n",
        "    print(\"\\n-------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "txODdF7Zkoib",
        "outputId": "01a30c1a-d761-48da-b574-892852f86c41"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tools in /usr/local/lib/python3.11/dist-packages (1.0.2)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error opening PDF 'Introduction_Why_Data_Science_Needs_Feminism.pdf': no such file: 'Introduction_Why_Data_Science_Needs_Feminism.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3702227382.py\u001b[0m in \u001b[0;36mextract_clean_text\u001b[0;34m(pdf_path, markers, save_to_file)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymupdf/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[1;32m   2956\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2957\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"no such file: '{filename}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2958\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: no such file: 'Introduction_Why_Data_Science_Needs_Feminism.pdf'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3702227382.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Script entry point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_to_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Preview (first 500 characters) ---\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-3702227382.py\u001b[0m in \u001b[0;36mextract_clean_text\u001b[0;34m(pdf_path, markers, save_to_file)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error opening PDF '{pdf_path}': {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error opening PDF 'Introduction_Why_Data_Science_Needs_Feminism.pdf': no such file: 'Introduction_Why_Data_Science_Needs_Feminism.pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. PDF Extraction & Sanitization\n",
        "\n",
        "**Purpose:** Extract text from the source PDF up to defined stop markers, then clean it by removing footnotes, page numbers, and other noise to prepare for further analysis.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  # Install dependencies\n",
        "  %pip install tools\n",
        "  %pip install PyMuPDF\n",
        "\n",
        "  # Imports for PDF parsing and sanitization\n",
        "  import fitz  # PyMuPDF for PDF reading\n",
        "  import re    # Regular expressions for text cleaning\n",
        "\n",
        "  # sanitize_text(text: str) -> str\n",
        "  #   Remove bracketed footnotes, back-reference arrows, numeric-only lines, page labels, and list markers.\n",
        "\n",
        "  # extract_clean_text(pdf_path, markers, save_to_file) -> str\n",
        "  #   Open PDF, read until a marker is found, concatenate page text, apply sanitize_text, optionally save to file.\n",
        "\n",
        "  # Script entry point for standalone execution\n",
        "  ```\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Ensure reliable extraction of main content by stopping at markers (e.g., 'references', 'footnotes').\n",
        "  * Use `PyMuPDF` for accurate, page-level text retrieval.\n",
        "  * Clean common artifacts (footnotes, page numbers) to improve text quality.\n",
        "  * Provide optional file output for auditing and iterative debugging.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  1. Run installation commands at the top of the notebook using Colab magics.\n",
        "  2. Import `fitz` and `re` for core functionality.\n",
        "  3. Define `sanitize_text` to apply regex-based cleaning and line filtering.\n",
        "  4. Define `extract_clean_text` to iterate through pages, detect stop markers, accumulate raw text, and clean.\n",
        "  5. Use the `__main__` guard to allow direct script execution and preview of cleaned text.\n",
        "\n"
      ],
      "metadata": {
        "id": "JaxIO8culTBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import json\n",
        "\n",
        "def load_text(path: str) -> str:\n",
        "    \"\"\"\n",
        "    Read cleaned text from a file and return it as a string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error loading file '{path}': {e}\")\n",
        "\n",
        "\n",
        "def tokenize_text(text: str) -> list:\n",
        "    \"\"\"\n",
        "    Perform linguistic analysis using spaCy and generate a list of token dictionaries with lemma, POS, and NER.\n",
        "    \"\"\"\n",
        "    nlp = spacy.load(SPACY_MODEL, disable=SPACY_DISABLE)\n",
        "    doc = nlp(text)\n",
        "\n",
        "    token_data = []\n",
        "    for token in doc:\n",
        "        # Skip stopwords\n",
        "        if token.is_stop:\n",
        "            continue\n",
        "        # Keep only alphabetic tokens\n",
        "        if not token.is_alpha:\n",
        "            continue\n",
        "        token_entry = {\n",
        "            \"word\": token.text,\n",
        "            \"lemma\": token.lemma_,\n",
        "            \"pos\": token.pos_,\n",
        "            \"ner\": token.ent_type_\n",
        "        }\n",
        "        token_data.append(token_entry)\n",
        "    return token_data\n",
        "\n",
        "\n",
        "def save_tokens_json(tokens: list, output_path: str):\n",
        "    \"\"\"\n",
        "    Save the token list as a JSON file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(tokens, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"ðŸ’¾ Tokens saved to: {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error saving tokens to '{output_path}': {e}\")\n",
        "\n",
        "\n",
        "def preview_tokens(tokens: list, limit: int = 10):\n",
        "    \"\"\"\n",
        "    Display a preview of the first N tokens.\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ§¾ Preview of first {limit} tokens:\")\n",
        "    for t in tokens[:limit]:\n",
        "        print(f\"{t['word']:15} | Lemma: {t['lemma']:15} | POS: {t['pos']:5} | NER: {t['ner']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸ§ª Starting tokenization...\")\n",
        "    raw_text = load_text(EXTRACTED_TEXT_FILE)\n",
        "    tokens = tokenize_text(raw_text)\n",
        "    print(f\"âœ… {len(tokens)} valid tokens extracted.\")\n",
        "    preview_tokens(tokens)\n",
        "    save_tokens_json(tokens, TOKENS_JSON_OUTPUT)\n"
      ],
      "metadata": {
        "id": "-usIjOTylT80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Text Loading & Tokenization\n",
        "\n",
        "**Purpose:** Read the cleaned text from file, perform NLP tokenization using spaCy, and manage token persistence and preview for further processing.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  import spacy\n",
        "  import json\n",
        "\n",
        "  def load_text(path: str) -> str:\n",
        "      # Read cleaned text file and return its content as a string\n",
        "      ...\n",
        "\n",
        "  def tokenize_text(text: str) -> list:\n",
        "      # Load spaCy model and generate a list of token dicts with word, lemma, POS, and NER\n",
        "      ...\n",
        "\n",
        "  def save_tokens_json(tokens: list, output_path: str):\n",
        "      # Serialize token list to a JSON file at the given path\n",
        "      ...\n",
        "\n",
        "  def preview_tokens(tokens: list, limit: int = 10):\n",
        "      # Print a preview of the first `limit` tokens for inspection\n",
        "      ...\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      # Orchestrate loading, tokenization, preview, and saving when run as a script\n",
        "      ...\n",
        "  ```\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * **Reproducible I/O:** Ensures consistent loading of preprocessed text.\n",
        "  * **Linguistic Analysis:** Extracts lexical features (lemmas, POS tags, named entities) for each token.\n",
        "  * **Noise Reduction:** Filters out stopwords and non-alphabetic tokens for clean vocabulary lists.\n",
        "  * **Persistence:** Saves token data in JSON format for downstream modules or manual inspection.\n",
        "  * **Feedback Loop:** Provides a quick console preview of tokens to validate processing steps.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  1. Call `load_text(EXTRACTED_TEXT_FILE)` to fetch cleaned text.\n",
        "  2. Initialize `nlp = spacy.load(SPACY_MODEL, disable=SPACY_DISABLE)` and process text.\n",
        "  3. Iterate over `nlp(text)` results, skipping stopwords and non-alpha tokens, and build `token_data`.\n",
        "  4. Use `json.dump(tokens, f)` to write `TOKENS_JSON_OUTPUT`.\n",
        "  5. Invoke `preview_tokens` to display the first few tokens and verify output.\n"
      ],
      "metadata": {
        "id": "7rh-JZA_lnmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages for PDF processing and translation\n",
        "%pip install tools\n",
        "%pip install PyMuPDF\n",
        "%pip install deep_translator\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_W-QRMtluXZ",
        "outputId": "f554bbfc-d806-4bbd-da99-bf2f67ee19a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tools in /usr/local/lib/python3.11/dist-packages (1.0.2)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.6.15)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep_translator\n",
            "Successfully installed deep_translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n",
        "\n",
        "**Purpose:** Ensure required packages are installed for PDF processing, text sanitization, and translation functionality.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  %pip install tools\n",
        "  %pip install PyMuPDF\n",
        "  %pip install deep_translator\n",
        "  ```\n",
        "\n",
        "  Install the `tools` utility package, `PyMuPDF` for PDF parsing, and `deep_translator` for translation services.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * `tools` provides helper functions across the project.\n",
        "  * `PyMuPDF` (imported as `fitz`) offers accurate PDF text extraction.\n",
        "  * `deep_translator` enables seamless integration with translation APIs (e.g., Google Translate).\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  * Execute these `%pip install` commands in the notebookâ€™s first cell.\n",
        "  * Use Colab magic commands to install packages into the active runtime without restarting the kernel.\n"
      ],
      "metadata": {
        "id": "iTWDXfLLmGil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# translator.py â€“ central module for translations with offline lookup, API fallback, and caching\n",
        "# ================================================================\n",
        "# Bundles all translation functions:\n",
        "# 1. Cache management to reuse previously translated words.\n",
        "# 2. Offline translation via WiktionaryParser (with fallback if methods are missing).\n",
        "# 3. API fallback via LibreTranslate.\n",
        "# 4. Return and update cache.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Optional import for offline Wiktionary parsing\n",
        "try:\n",
        "    from wiktionaryparser import WiktionaryParser\n",
        "except ImportError:\n",
        "    WiktionaryParser = None\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Load translation cache from file\n",
        "# ----------------------------------------------------------------\n",
        "def _load_cache() -> dict:\n",
        "    if os.path.exists(TRANSLATION_CACHE_FILE):\n",
        "        try:\n",
        "            with open(TRANSLATION_CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Save translation cache to file\n",
        "# ----------------------------------------------------------------\n",
        "def _save_cache(cache: dict):\n",
        "    os.makedirs(os.path.dirname(TRANSLATION_CACHE_FILE) or '.', exist_ok=True)\n",
        "    with open(TRANSLATION_CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cache, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Attempt offline translation via WiktionaryParser\n",
        "# ----------------------------------------------------------------\n",
        "def _translate_offline(word: str) -> str:\n",
        "    if WiktionaryParser is None:\n",
        "        return \"\"\n",
        "\n",
        "    parser = WiktionaryParser()\n",
        "    try:\n",
        "        parser.set_default_language(\"english\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        parser.set_languages([\"german\"])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        entries = parser.fetch(word)\n",
        "        if entries:\n",
        "            definitions = entries[0].get(\"definitions\", []) or []\n",
        "            for d in definitions:\n",
        "                translations = d.get(\"translations\") or {}\n",
        "                german = translations.get(\"german\") or translations.get(\"deutsch\")\n",
        "                if german:\n",
        "                    return \", \".join(german)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"\"\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# API fallback via LibreTranslate\n",
        "# ----------------------------------------------------------------\n",
        "def _translate_api(word: str) -> str:\n",
        "    if FALLBACK_SERVICE.lower() != \"libre\":\n",
        "        return \"\"\n",
        "    try:\n",
        "        resp = requests.post(\n",
        "            \"https://libretranslate.com/translate\",\n",
        "            data={\"q\": word, \"source\": \"en\", \"target\": \"de\", \"format\": \"text\"},\n",
        "            timeout=10\n",
        "        )\n",
        "        if resp.ok:\n",
        "            return resp.json().get(\"translatedText\", \"\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"\"\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Main translation function: translate_word\n",
        "# ----------------------------------------------------------------\n",
        "def translate_word(word: str) -> str:\n",
        "    cache = _load_cache()\n",
        "    key = word.strip().lower()\n",
        "    # 1) Check cache\n",
        "    if key in cache:\n",
        "        return cache[key]\n",
        "\n",
        "    # 2) Primary translation via GoogleTranslator\n",
        "    translation = GoogleTranslator(source='en', target='de').translate(word)\n",
        "\n",
        "    # 3) (Optional) Offline or API fallback\n",
        "    # if TRANSLATION_SERVICE.lower() == \"wiktionary\":\n",
        "    #     offline = _translate_offline(word)\n",
        "    #     if offline:\n",
        "    #         translation = offline\n",
        "    # if not translation and ENABLE_FALLBACK:\n",
        "    #     translation = _translate_api(word)\n",
        "\n",
        "    # 4) Update and save cache\n",
        "    cache[key] = translation\n",
        "    _save_cache(cache)\n",
        "\n",
        "    return translation\n"
      ],
      "metadata": {
        "id": "hbxb-Yf9mSSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Translation Module (translator.py)\n",
        "\n",
        "**Purpose:** Centralize word translation logic with caching, offline lookup via Wiktionary, and online API fallback to ensure efficient and reliable Englishâ†’German translations.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  import os\n",
        "  import json\n",
        "  import requests\n",
        "  from deep_translator import GoogleTranslator\n",
        "  try:\n",
        "      from wiktionaryparser import WiktionaryParser\n",
        "  except ImportError:\n",
        "      WiktionaryParser = None\n",
        "\n",
        "  def _load_cache() -> dict: ...\n",
        "  def _save_cache(cache: dict): ...\n",
        "  def _translate_offline(word: str) -> str: ...\n",
        "  def _translate_api(word: str) -> str: ...\n",
        "  def translate_word(word: str) -> str: ...\n",
        "  ```\n",
        "\n",
        "  Functions handle cache I/O, offline Wiktionary lookup, API calls to LibreTranslate, and orchestration via GoogleTranslator with transparent fallback.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * `_load_cache / _save_cache`: Reuse previous translations and minimize API usage.\n",
        "  * `_translate_offline`: Attempt local dictionary lookup to reduce network dependency.\n",
        "  * `_translate_api`: Provide online fallback when offline lookup fails.\n",
        "  * `translate_word`: Offer a single entry point that checks cache, uses GoogleTranslator, and updates the cache.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  1. Load translation cache from `TRANSLATION_CACHE_FILE`.\n",
        "  2. If the word exists in the cache, return the cached result.\n",
        "  3. Use `GoogleTranslator(source='en', target='de')` for primary translation.\n",
        "  4. (Optional) Uncomment offline or API fallback code to enable additional lookup methods.\n",
        "  5. Store the final translation in the cache and save it to disk.\n"
      ],
      "metadata": {
        "id": "zu0o3IALn0t-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KYMProTEoTc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# ðŸ“„ step3a_cefr_parser.py â€“ Oxford 5000 PDF Parsing and Filtering\n",
        "# ===============================================================\n",
        "# Extract B2 and C1 level vocabulary from the \"The Oxford 5000 by CEFR level\" PDF,\n",
        "# clean footers, parse word + POS, and write results to CSV per CSV_COLUMNS in config.py.\n",
        "# ===============================================================\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import csv\n",
        "\n",
        "# Path to the Oxford 5000 PDF\n",
        "PDF_PATH = \"The_Oxford_5000_by_CEFR_level.pdf\"\n",
        "# Output CSV file for filtered vocabulary\n",
        "OUTPUT_CSV = \"oxford_5000_cefr.csv\"\n",
        "\n",
        "# Regex to remove footers (e.g., \"Â© Oxford University Press The Oxford 5000â„¢ by CEFR level 2 / 8\")\n",
        "FOOTER_PATTERN = re.compile(r\"Â© Oxford University Press.*\\d+\\s*/\\s*\\d+\")\n",
        "\n",
        "# Regex for entries: word followed by POS notation, e.g., \"absorb v.\" or \"boost v., n.\"\n",
        "ENTRY_PATTERN = re.compile(r\"^([a-zA-Z\\-]+)\\s+([a-z\\.,\\s]+)\\.\")\n",
        "\n",
        "# CEFR levels to extract\n",
        "VALID_LEVELS = {\"B2\", \"C1\"}\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Load the PDF and extract raw text, removing defined footer patterns.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    all_text = []\n",
        "\n",
        "    for page in doc:\n",
        "        text = page.get_text(\"text\")\n",
        "        # Remove footer if present\n",
        "        text = FOOTER_PATTERN.sub(\"\", text)\n",
        "        all_text.append(text.strip())\n",
        "\n",
        "    doc.close()\n",
        "    return \"\\n\".join(all_text)\n",
        "\n",
        "def parse_cefr_vocab(text: str) -> list:\n",
        "    \"\"\"\n",
        "    Parse the extracted text line by line, detect level markers and entries,\n",
        "    and build a list of vocabulary entry dicts.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    current_level = None\n",
        "\n",
        "    for line in text.splitlines():\n",
        "        line = line.strip()\n",
        "        # Skip empty lines and headings\n",
        "        if not line or line.lower().startswith(\"the oxford\"):\n",
        "            continue\n",
        "\n",
        "        # Detect level markers\n",
        "        if line in VALID_LEVELS:\n",
        "            current_level = line\n",
        "            continue\n",
        "\n",
        "        # Match vocabulary entries\n",
        "        match = ENTRY_PATTERN.match(line)\n",
        "        if match and current_level in VALID_LEVELS:\n",
        "            word = match.group(1)\n",
        "            pos_raw = match.group(2).replace(\" \", \"\")\n",
        "            pos_list = [pos.strip() for pos in pos_raw.split(\",\") if pos.strip()]\n",
        "\n",
        "            for pos in pos_list:\n",
        "                entry = {\n",
        "                    \"word\": word,\n",
        "                    \"lemma\": \"\",\n",
        "                    \"pos\": pos,\n",
        "                    \"ner\": \"\",\n",
        "                    \"cefr_predicted\": current_level,\n",
        "                    \"translation_de\": \"\",\n",
        "                    \"anki_front\": \"\",\n",
        "                    \"anki_back\": \"\",\n",
        "                    \"anki_type\": \"\",\n",
        "                    \"status_learning\": \"new\",\n",
        "                    \"repetition_stage\": \"\"\n",
        "                }\n",
        "                rows.append(entry)\n",
        "\n",
        "    return rows\n",
        "\n",
        "def write_csv(rows: list, csv_path: str):\n",
        "    \"\"\"\n",
        "    Write the vocabulary rows to a CSV file using CSV_COLUMNS from config.\n",
        "    \"\"\"\n",
        "    with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
        "        writer = csv.DictWriter(f_out, fieldnames=CSV_COLUMNS)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "\n",
        "    print(f\"âœ… Completed! Wrote {len(rows)} entries to '{csv_path}'\")\n",
        "\n",
        "# -------------------- Main Script --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸ“¥ Extracting text from PDF...\")\n",
        "    pdf_text = extract_text_from_pdf(PDF_PATH)\n",
        "\n",
        "    print(\"ðŸ§© Parsing CEFR vocabulary entries...\")\n",
        "    vocab_entries = parse_cefr_vocab(pdf_text)\n",
        "\n",
        "    print(\"ðŸ’¾ Writing entries to CSV...\")\n",
        "    write_csv(vocab_entries, OUTPUT_CSV)\n"
      ],
      "metadata": {
        "id": "m7oPsaXdotBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## step3a\\_cefr\\_parser.py â€“ Oxford 5000 PDF Parsing and Filtering\n",
        "\n",
        "**Purpose:** Extract and filter B2/C1 level vocabulary entries from the â€œThe Oxford 5000 by CEFR levelâ€ PDF, clean page footers, and export structured entries to CSV using the schema defined in `config.py`.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  # Open PDF and remove footer patterns\n",
        "  def extract_text_from_pdf(pdf_path: str) -> str: ...\n",
        "\n",
        "  # Parse cleaned text, detect CEFR level markers, and build vocabulary entries\n",
        "  def parse_cefr_vocab(text: str) -> list: ...\n",
        "\n",
        "  # Write the resulting entries to CSV using CSV_COLUMNS\n",
        "  def write_csv(rows: list, csv_path: str): ...\n",
        "\n",
        "  # Main script: orchestrate extraction, parsing, and CSV writing\n",
        "  ```\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Automate extraction of target-level vocabulary (B2 and C1) from a standardized CEFR list.\n",
        "  * Remove recurring footer noise to prevent false matches and ensure data cleanliness.\n",
        "  * Structure output in CSV format for seamless integration with quiz and flashcard modules.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  1. Load the PDF using `fitz.open()` and extract raw text, applying `FOOTER_PATTERN` to strip out footers.\n",
        "  2. Iterate over each line of text: identify level markers, apply `ENTRY_PATTERN` to match word + POS, and accumulate entries in a list of dictionaries.\n",
        "  3. Use `csv.DictWriter` with `CSV_COLUMNS` to serialize the list of entry dicts into the output CSV file.\n",
        "  4. Run as a standalone script, printing progress messages at each stage.\n"
      ],
      "metadata": {
        "id": "ol3_vSdGot1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# ðŸ“„ step3aa_translate_oxford.py â€“ One-time Oxford Vocabulary Translation\n",
        "# ===============================================================\n",
        "# Reads 'oxford_5000_cefr.csv', translates each 'word' field into German,\n",
        "# and writes the output to 'oxford_5000_translated.csv'.\n",
        "# Uses primary translation via GoogleTranslator, with optional offline or API fallback.\n",
        "# ===============================================================\n",
        "\n",
        "import csv\n",
        "\n",
        "INPUT_CSV = \"oxford_5000_cefr.csv\"\n",
        "OUTPUT_CSV = \"oxford_5000_translated.csv\"\n",
        "\n",
        "\n",
        "def load_vocab(csv_path: str) -> list:\n",
        "    \"\"\"\n",
        "    Read vocabulary entries from a CSV file and return them as a list of dictionaries.\n",
        "    \"\"\"\n",
        "    with open(csv_path, encoding=\"utf-8\") as f_in:\n",
        "        reader = csv.DictReader(f_in)\n",
        "        return list(reader)\n",
        "\n",
        "\n",
        "def translate_vocab(rows: list) -> list:\n",
        "    \"\"\"\n",
        "    Translate entries where 'translation_de' is empty, skipping already translated words.\n",
        "    \"\"\"\n",
        "    for row in rows:\n",
        "        word = row[\"word\"].strip()\n",
        "        if row.get(\"translation_de\"):\n",
        "            continue\n",
        "        try:\n",
        "            row[\"translation_de\"] = translate_word(word)\n",
        "        except Exception as e:\n",
        "            row[\"translation_de\"] = \"[ERROR]\"\n",
        "            print(f\"âš ï¸ Error translating '{word}': {e}\")\n",
        "    return rows\n",
        "\n",
        "\n",
        "def save_vocab(rows: list, csv_path: str):\n",
        "    \"\"\"\n",
        "    Save the translated vocabulary list to a new CSV file.\n",
        "    \"\"\"\n",
        "    with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f_out:\n",
        "        writer = csv.DictWriter(f_out, fieldnames=CSV_COLUMNS)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "    print(f\"âœ… Translated file saved to: {csv_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸ“¥ Loading Oxford vocabulary list...\")\n",
        "    data = load_vocab(INPUT_CSV)\n",
        "    print(\"ðŸŒ Translating vocabulary entries...\")\n",
        "    translated = translate_vocab(data)\n",
        "    print(\"ðŸ’¾ Saving translated list...\")\n",
        "    save_vocab(translated, OUTPUT_CSV)\n"
      ],
      "metadata": {
        "id": "IjGafP7zo25q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step3aa_translate_oxford.py â€“ One-time Oxford Vocabulary Translation\n",
        "\n",
        "Purpose: Translate B2/C1 vocabulary entries from the extracted Oxford list into German and export results for downstream usage.\n",
        "\n",
        "What:\n",
        "\n",
        "import csv\n",
        "INPUT_CSV = \"oxford_5000_cefr.csv\"\n",
        "OUTPUT_CSV = \"oxford_5000_translated.csv\"\n",
        "\n",
        "def load_vocab(csv_path: str) -> list: ...\n",
        "def translate_vocab(rows: list) -> list: ...\n",
        "def save_vocab(rows: list, csv_path: str): ...\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Orchestrate loading, translating, and saving\n",
        "\n",
        "Why:\n",
        "\n",
        "Automate translation of extracted vocabulary without manual effort.\n",
        "\n",
        "Skip entries already translated to minimize API calls.\n",
        "\n",
        "Handle translation errors gracefully with placeholders.\n",
        "\n",
        "Produce a cleaned CSV file ready for integration into quiz or flashcard modules.\n",
        "\n",
        "How:\n",
        "\n",
        "Read input CSV using csv.DictReader.\n",
        "\n",
        "Loop through rows, calling translate_word for each word needing translation.\n",
        "\n",
        "Populate translation_de field and catch exceptions.\n",
        "\n",
        "Write updated entries to OUTPUT_CSV with csv.DictWriter.\n",
        "\n",
        "Execute logic in the __main__ block, printing status messages at each step.\n",
        "\n"
      ],
      "metadata": {
        "id": "_o_6RXRBpEdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# ðŸ“„ step3b_active-pdf-translate.py â€“ Complete Step 3b: Oxford List, Token Loading & Translation\n",
        "# ===============================================================\n",
        "# Combines the Oxford CEFR list with token data, translates new tokens, and exports an extended vocabulary CSV.\n",
        "# ===============================================================\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# Configure logging for translation errors\n",
        "logging.basicConfig(\n",
        "    filename='translation_errors.log',\n",
        "    filemode='a',\n",
        "    format='%(asctime)s %(levelname)s: %(message)s',\n",
        "    level=logging.WARNING\n",
        ")\n",
        "\n",
        "# Load or initialize translation cache\n",
        "cache_path = Path(TRANSLATION_CACHE_FILE)\n",
        "translation_cache = {}\n",
        "if cache_path.exists():\n",
        "    try:\n",
        "        translation_cache = json.loads(cache_path.read_text(encoding='utf-8'))\n",
        "    except json.JSONDecodeError:\n",
        "        translation_cache = {}\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Load Oxford vocabulary as a set\n",
        "# ----------------------------------------------------------------\n",
        "def load_oxford_vocab(csv_path: str) -> set:\n",
        "    vocab = set()\n",
        "    with open(csv_path, newline='', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            word = row.get('word')\n",
        "            if word:\n",
        "                vocab.add(word)\n",
        "    return vocab\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Get translation with caching and logging\n",
        "# ----------------------------------------------------------------\n",
        "def get_translation(word: str) -> str:\n",
        "    key = word.lower()\n",
        "    if key in translation_cache:\n",
        "        return translation_cache[key].get('translation_de', '')\n",
        "    try:\n",
        "        translation = GoogleTranslator(source='en', target='de').translate(word)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"GoogleTranslator failed for {word}: {e}\")\n",
        "        translation = ''\n",
        "    translation_cache[key] = {'translation_de': translation}\n",
        "    return translation\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Enrich token list with translations for unknown words\n",
        "# ----------------------------------------------------------------\n",
        "def enrich_tokens(tokens: list, known_vocab: set) -> list:\n",
        "    new_entries = []\n",
        "    for token in tokens:\n",
        "        word = token['word']\n",
        "        if word in known_vocab:\n",
        "            continue\n",
        "        translation = get_translation(word)\n",
        "        if not translation:\n",
        "            print(f\"âš ï¸ No translation for '{word}', skipping.\")\n",
        "            continue\n",
        "        entry = {\n",
        "            'word': word,\n",
        "            'lemma': token.get('lemma', ''),\n",
        "            'pos': token.get('pos', ''),\n",
        "            'ner': token.get('ner', ''),\n",
        "            'cefr_predicted': '',\n",
        "            'translation_de': translation,\n",
        "            'anki_front': '',\n",
        "            'anki_back': '',\n",
        "            'anki_type': '',\n",
        "            'status_learning': 'new',\n",
        "            'repetition_stage': ''\n",
        "        }\n",
        "        new_entries.append(entry)\n",
        "        print(f\"Translated: {word} â†’ {translation}\")\n",
        "        time.sleep(0.2)\n",
        "    return new_entries\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Save translation cache back to file\n",
        "# ----------------------------------------------------------------\n",
        "def save_cache():\n",
        "    with open(TRANSLATION_CACHE_FILE, 'w', encoding='utf-8') as f:\n",
        "        json.dump(translation_cache, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"ðŸ’¾ Cache saved to '{TRANSLATION_CACHE_FILE}'\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Save enriched entries to CSV\n",
        "# ----------------------------------------------------------------\n",
        "def save_to_csv(entries: list, csv_path: str):\n",
        "    with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=CSV_COLUMNS)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(entries)\n",
        "    print(f\"âœ… Saved {len(entries)} entries to '{csv_path}'\")\n",
        "\n",
        "# -------------------- Main --------------------\n",
        "def main():\n",
        "    oxford_csv = 'oxford_5000_cefr.csv'\n",
        "    tokens_json = 'tokens_step2.json'\n",
        "    output_csv = 'extended_vocab.csv'\n",
        "\n",
        "    with open(tokens_json, 'r', encoding='utf-8') as f:\n",
        "        tokens = json.load(f)\n",
        "\n",
        "    print('ðŸ” Loading Oxford vocabulary...')\n",
        "    known_vocab = load_oxford_vocab(oxford_csv)\n",
        "\n",
        "    print('ðŸ”„ Enriching tokens with translations...')\n",
        "    new_entries = enrich_tokens(tokens, known_vocab)\n",
        "\n",
        "    print('ðŸ’¾ Saving enriched vocabulary to CSV...')\n",
        "    save_to_csv(new_entries, output_csv)\n",
        "\n",
        "    save_cache()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-XB-sYt_pGUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step3b_active-pdf-translate.py â€“ Combined Vocabulary Enrichment\n",
        "\n",
        "Purpose: Integrate Oxford CEFR list and tokenized text to translate and export new vocabulary entries, extending the core dataset.\n",
        "\n",
        "What:\n",
        "\n",
        "import csv, json, time, logging\n",
        "from pathlib import Path\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "# load_oxford_vocab(csv_path) -> set\n",
        "# get_translation(word) -> str\n",
        "# enrich_tokens(tokens, known_vocab) -> list\n",
        "# save_cache() -> None\n",
        "# save_to_csv(entries, csv_path) -> None\n",
        "# main() orchestrates the above steps\n",
        "\n",
        "Functions handle loading existing vocabulary, caching translations, enriching tokens, and persisting results.\n",
        "\n",
        "Why:\n",
        "\n",
        "Leverage existing Oxford list to avoid duplicate translations.\n",
        "\n",
        "Cache results to minimize API calls and handle transient failures.\n",
        "\n",
        "Provide logging of translation errors for later review.\n",
        "\n",
        "Generate an extended vocabulary CSV for quiz and flashcard modules.\n",
        "\n",
        "How:\n",
        "\n",
        "Initialize logging and load translation cache.\n",
        "\n",
        "Load existing Oxford words into a set via load_oxford_vocab.\n",
        "\n",
        "Read tokens from tokens_step2.json.\n",
        "\n",
        "For each token not in the known set, call get_translation and build entry dicts.\n",
        "\n",
        "Write enriched entries to extended_vocab.csv and save cache back to TRANSLATION_CACHE_FILE.\n",
        "\n",
        "Execute all logic within a main() function guarded by if __name__ == '__main__'.\n",
        "\n"
      ],
      "metadata": {
        "id": "8XTzhr3VpRrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install genanki\n",
        "\n",
        "**Purpose:** Ensure the `genanki` library is available for programmatic creation of Anki flashcard decks.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  %pip install genanki\n",
        "  ```\n",
        "\n",
        "  Install the `genanki` package into the current notebook environment.\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * Enables automated generation of spaced-repetition flashcards.\n",
        "  * Integrates with the projectâ€™s export scripts to produce `.apkg` files directly from code.\n",
        "  * Simplifies the workflow by removing manual steps for Anki deck creation.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  1. Run the `%pip install genanki` command in a code cell at the start of the notebook.\n",
        "  2. Colab magic ensures installation into the active kernel without restarting the runtime.\n"
      ],
      "metadata": {
        "id": "Rvt0ps-_qUXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from genanki import Model, Note, Deck, Package\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 1. Load translated CSV (from step3b/3aa)\n",
        "# ---------------------------------------------------------------\n",
        "def load_translated_csv(path: str) -> list:\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        return [\n",
        "            row for row in reader\n",
        "            if row.get(\"translation_de\") and not row[\"translation_de\"].startswith(\"[\")\n",
        "        ]\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 2. Export minimal CSV (english/german)\n",
        "# ---------------------------------------------------------------\n",
        "def export_to_csv(data: list, path: str):\n",
        "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"english\", \"german\"])\n",
        "        for item in data:\n",
        "            writer.writerow([item[\"word\"], item[\"translation_de\"]])\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 3. Export Anki deck (.apkg) via genanki\n",
        "# ---------------------------------------------------------------\n",
        "def export_to_anki(data: list, deck_name: str, output_file: str):\n",
        "    model = Model(\n",
        "        1607392319,\n",
        "        \"Simple Model\",\n",
        "        fields=[{\"name\": \"Question\"}, {\"name\": \"Answer\"}],\n",
        "        templates=[{\n",
        "            \"name\": \"Card 1\",\n",
        "            \"qfmt\": \"{{Question}}\",\n",
        "            \"afmt\": \"{{FrontSide}}<hr id='answer'>{{Answer}}\",\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    deck = Deck(2059400110, deck_name)\n",
        "\n",
        "    for item in data:\n",
        "        note = Note(\n",
        "            model=model,\n",
        "            fields=[item[\"word\"], item[\"translation_de\"]]\n",
        "        )\n",
        "        deck.add_note(note)\n",
        "\n",
        "    Package(deck).write_to_file(output_file)\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# â–¶ï¸ Main (script entry)\n",
        "# ---------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸ§ª Starting Anki export...\")\n",
        "\n",
        "    # 1. Load translated vocabulary\n",
        "    tokens = load_translated_csv(\"oxford_5000_translated.csv\")\n",
        "    print(f\"ðŸ“š {len(tokens)} translated entries loaded.\")\n",
        "\n",
        "    # 2. Export to simple CSV for manual use\n",
        "    export_to_csv(tokens, \"anki_cards.csv\")\n",
        "    print(\"âœ… CSV saved: anki_cards.csv\")\n",
        "\n",
        "    # 3. Create Anki deck\n",
        "    export_to_anki(tokens, \"English-German Vocabulary Training\", \"anki_deck.apkg\")\n",
        "    print(\"ðŸŽ‰ Anki deck saved: anki_deck.apkg\")\n"
      ],
      "metadata": {
        "id": "0oX_wBldqVTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## step3c\\_anki\\_export.py â€“ CSV and Anki Deck Export\n",
        "\n",
        "**Purpose:** Generate a simple bilingual CSV and a fully formatted Anki deck (`.apkg`) from translated vocabulary entries to support interactive learning.\n",
        "\n",
        "* **What:**\n",
        "\n",
        "  ```python\n",
        "  import csv\n",
        "  from genanki import Model, Note, Deck, Package\n",
        "\n",
        "  def load_translated_csv(path: str) -> list: ...\n",
        "  def export_to_csv(data: list, path: str): ...\n",
        "  def export_to_anki(data: list, deck_name: str, output_file: str): ...\n",
        "\n",
        "  if __name__ == \"__main__\":\n",
        "      # Load translated entries, save CSV, create Anki deck\n",
        "  ```\n",
        "\n",
        "* **Why:**\n",
        "\n",
        "  * **load\\_translated\\_csv:** Filter out entries lacking valid translations to focus on usable vocabulary.\n",
        "  * **export\\_to\\_csv:** Provide a lightweight CSV of Englishâ€“German pairs for manual review or import into other systems.\n",
        "  * **export\\_to\\_anki:** Automate creation of a spaced-repetition deck in Anki format, removing manual deck-building steps.\n",
        "\n",
        "* **How:**\n",
        "\n",
        "  1. Read the translated CSV using `csv.DictReader` and filter rows where `translation_de` is non-empty and not marked as error.\n",
        "  2. Write a minimal CSV with headers `english,german` via `csv.writer`.\n",
        "  3. Define an Anki `Model` with basic front/back fields and a template.\n",
        "  4. Create a `Deck` and iterate through data to add `Note` objects with question-answer fields.\n",
        "  5. Use `genanki.Package(deck).write_to_file(output_file)` to save the deck as `.apkg`.\n"
      ],
      "metadata": {
        "id": "3IDRo-I7rsud"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "la0tWTBFrz_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LUG4xw7UpT49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install genanki"
      ],
      "metadata": {
        "id": "FrcgZtqCphQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qzsH5ukBph_8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "we_FzDxUmHHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "safpDiHnlKTz"
      }
    }
  ]
}